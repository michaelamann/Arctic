---
title: "USEARCH_commands"
author: "Michael Mann"
date: "5/3/2022"
output: html_document
---


This is just a record of the commands run on CARC (UNM's cluster)
```{r setup}
library(tidyverse)
```



I already processed the unique sequences for Ina's work so i will start with that file. Used parameters for unoise3 (I only have license for USEARCH9)
Stored in ZOTUs folder locally. on CARC: /users/mimann/wheeler-scratch/arctic/clustering
unoise_2022.slurm job. Finished April 29th 2022
output: zotus.fa, unoise3.txt (no output which is odd but not needed), slurm-91226.out
```{bash unoise_2022.slurm }
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=24:00:00
#SBATCH --job-name=unoise


cd /users/mimann/wheeler-scratch/arctic/clustering

# using unoise2 but with unoise3 parameters
/users/mimann/usearch9 -unoise2 Arctic_Fungi_uniques.fa -fastaout zotus.fa -tabbedout unoise3.txt -minampsize 8 -abskew 16


```


Given that some zotus could be the more abundant but subets of other zotus, I blasted them at 99% to find all of their hits. Will clean them up in a script (removing duplicate zotus). 

Stored in ZOTUs folder locally. on CARC: /users/mimann/wheeler-scratch/arctic/clustering
cleaning_up_zotus.slurm job. Finished May 3rd 2022
output: zotus_duplicate_hits.b6, slurm-96069.out
```{bash cleaning_up_zotus.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=00:10:00
#SBATCH --job-name=cleanup_zotus
#SBATCH --partition=debug

cd /users/mimann/wheeler-scratch/arctic/clustering

# using unoise2 but with unoise3 parameters
/users/mimann/usearch9 -usearch_global zotus.fa -db zotus.fa -id 0.99 -blast6out zotus_duplicate_hits.b6 -strand both -maxaccepts 500 -threads 8

```

mapping back otus using the cleaned up zotu file. 

Stored in ZOTUs folder locally. on CARC: /users/mimann/wheeler-scratch/arctic/clustering
mapping_zotus.slurm job. Finished May 3rd 2022
output: zotu_table.txt, slurm-96270.out
```{bash mapping_zotus.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --job-name=map_zotus

# note this is in a different folder than the rest of the analyses. 
cd /users/mimann/wheeler-scratch/arctic/clustering

# using the filtered data so its slightly more high quality for mapping. 
# also a much smaller file
# gunzip arctic_filtered.fasta.gz


/users/mimann/usearch9 -usearch_global arctic_filtered.fasta -db zotus_removed_duplicates.fa -otutabout zotu_table.txt -threads 8 -id 0.995 -strand plus
```

Also figuring out taxonomy


output: zotu_taxonomy.txt, slurm-96836.out
```{bash zotu_taxonomy.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=01:00:00
#SBATCH --job-name=zotu_taxonomy
#SBATCH --partition=debug

# note this is in a different folder than the rest of the analyses. 
cd /users/mimann/wheeler-scratch/arctic/clustering

/users/mimann/usearch9 -sintax zotus_removed_duplicates.fa -db /users/mimann/wheeler-scratch/rmbl_seq_clustering_Jan_2020/data/utax_reference_dataset_all_04.02.2020_corrected.fasta -tabbedout zotu_taxonomy.txt -strand both -sintax_cutoff 0.8
```


Checking which zOTUs match up with Leho's OTUs
output: zotus_duplicate_hits_tedersoo.b6, slurm-103614.out
```{bash tedersoo_hits.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --job-name=map_Tedersoo

# note this is in a different folder than the rest of the analyses. 
cd /users/mimann/wheeler-scratch/arctic/clustering


/users/mimann/usearch9 -usearch_global zotus_removed_duplicates.fa -db Tedersoo_Fungi_GSMc_OTUs.fasta -otutabout zotu_table.txt -threads 8  -blast6out zotus_duplicate_hits_tedersoo.b6 -strand both -maxaccepts 500 -threads 8

```


checking which zotus map back to long reads. Helpful to know which samples to prioritize. 
output:zotus_duplicate_hits_pacbio.b6, zotus_duplicate_hits_sanger.b6
```{bash long_read_hits.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --job-name=map_long_Reads

# note this is in a different folder than the rest of the analyses. 
cd /users/mimann/wheeler-scratch/arctic/clustering


/users/mimann/usearch9 -usearch_global zotus_removed_duplicates.fa -db /users/mimann/wheeler-scratch/pacbio/run_2/uniques.fasta -threads 8  -blast6out zotus_duplicate_hits_pacbio.b6 -strand both -maxaccepts 500 -threads 8 -id 1.00


/users/mimann/usearch9 -usearch_global zotus_removed_duplicates.fa -db /users/mimann/wheeler-scratch/arctic/matching_long_reads/all_arctic_sanger.fasta -threads 8  -blast6out zotus_duplicate_hits_sanger.b6 -strand both -maxaccepts 500 -threads 8 -id 1.00

```

running uparse at 98% so i can use this for removing chimeras prior to clustering against Leho dataset
-------
rerunning clustering at 98%. i will need to download the uparse output and manually filter out the chimeras. Then I will 
```{bash   chimera_detection.slurm}


#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --mem=0
#SBATCH --time=4:00:00
#SBATCH --job-name=finding_chimeras

cd /users/mimann/wheeler-scratch/arctic/clustering


/users/mimann/usearch8.1.1861_i86linux32 -cluster_otus Arctic_Fungi_uniques.fa -uparseout uparse.txt -otu_radius_pct .98
```



```{r clean up fasta to remove chimeras}
library(microseq)
library(tidyverse)

uparse_calls <- read_table("Chimera_removal/uparse.txt", col_names = c("Read", "Call", "Percent", "Extra"))

unique_reads <- readFasta("Chimera_removal/Arctic_Fungi_uniques.fa")

# find all chimeras from uparse file
chimeras <- 
  uparse_calls %>%
  filter(Call != "otu" & Call != "match" )  %>%
  pull(Read)

# remove chimeras from unique reads
# write to file 
unique_reads %>%
  filter(!(Header %in% chimeras)) %>%
  writeFasta("Chimera_removal/unique_reads_non_chimeras.fa")



```



# in case i want an interactive node
salloc --cpus-per-task=8 --time=02:00:00 --partition=debug