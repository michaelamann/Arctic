---
title: "USEARCH_commands"
author: "Michael Mann"
date: "5/3/2022"
output: html_document
---


This is just a record of the commands run on CARC (UNM's cluster)
```{r setup}
library(tidyverse)
```



I already processed the unique sequences for Ina's work so i will start with that file. Used parameters for unoise3 (I only have license for USEARCH9)
Stored in ZOTUs folder locally. on CARC: /users/mimann/wheeler-scratch/arctic/clustering
unoise_2022.slurm job. Finished April 29th 2022
output: zotus.fa, unoise3.txt (no output which is odd but not needed), slurm-91226.out
```{bash unoise_2022.slurm }
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=24:00:00
#SBATCH --job-name=unoise


cd /users/mimann/wheeler-scratch/arctic/clustering

# using unoise2 but with unoise3 parameters
/users/mimann/usearch9 -unoise2 Arctic_Fungi_uniques.fa -fastaout zotus.fa -tabbedout unoise3.txt -minampsize 8 -abskew 16


```


Given that some zotus could be the more abundant but subets of other zotus, I blasted them at 99% to find all of their hits. Will clean them up in a script (removing duplicate zotus). 

Stored in ZOTUs folder locally. on CARC: /users/mimann/wheeler-scratch/arctic/clustering
cleaning_up_zotus.slurm job. Finished May 3rd 2022
output: zotus_duplicate_hits.b6, slurm-96069.out
```{bash cleaning_up_zotus.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=00:10:00
#SBATCH --job-name=cleanup_zotus
#SBATCH --partition=debug

cd /users/mimann/wheeler-scratch/arctic/clustering

# using unoise2 but with unoise3 parameters
/users/mimann/usearch9 -usearch_global zotus.fa -db zotus.fa -id 0.99 -blast6out zotus_duplicate_hits.b6 -strand both -maxaccepts 500 -threads 8

```

mapping back otus using the cleaned up zotu file. 

Stored in ZOTUs folder locally. on CARC: /users/mimann/wheeler-scratch/arctic/clustering
mapping_zotus.slurm job. Finished May 3rd 2022
output: zotu_table.txt, slurm-96270.out
```{bash mapping_zotus.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --job-name=map_zotus

# note this is in a different folder than the rest of the analyses. 
cd /users/mimann/wheeler-scratch/arctic/clustering

# using the filtered data so its slightly more high quality for mapping. 
# also a much smaller file
# gunzip arctic_filtered.fasta.gz


/users/mimann/usearch9 -usearch_global arctic_filtered.fasta -db zotus_removed_duplicates.fa -otutabout zotu_table.txt -threads 8 -id 0.995 -strand plus
```

Also figuring out taxonomy


output: zotu_taxonomy.txt, slurm-96836.out
```{bash zotu_taxonomy.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=01:00:00
#SBATCH --job-name=zotu_taxonomy
#SBATCH --partition=debug

# note this is in a different folder than the rest of the analyses. 
cd /users/mimann/wheeler-scratch/arctic/clustering

/users/mimann/usearch9 -sintax zotus_removed_duplicates.fa -db /users/mimann/wheeler-scratch/rmbl_seq_clustering_Jan_2020/data/utax_reference_dataset_all_04.02.2020_corrected.fasta -tabbedout zotu_taxonomy.txt -strand both -sintax_cutoff 0.8
```


Checking which zOTUs match up with Leho's OTUs
output: zotus_duplicate_hits_tedersoo.b6, slurm-103614.out
```{bash tedersoo_hits.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --job-name=map_Tedersoo

# note this is in a different folder than the rest of the analyses. 
cd /users/mimann/wheeler-scratch/arctic/clustering


/users/mimann/usearch9 -usearch_global zotus_removed_duplicates.fa -db Tedersoo_Fungi_GSMc_OTUs.fasta -otutabout zotu_table.txt -threads 8  -blast6out zotus_duplicate_hits_tedersoo.b6 -strand both -maxaccepts 500 -threads 8

```


checking which zotus map back to long reads. Helpful to know which samples to prioritize. 
output:zotus_duplicate_hits_pacbio.b6, zotus_duplicate_hits_sanger.b6
```{bash long_read_hits.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --job-name=map_long_Reads

# note this is in a different folder than the rest of the analyses. 
cd /users/mimann/wheeler-scratch/arctic/clustering


/users/mimann/usearch9 -usearch_global zotus_removed_duplicates.fa -db /users/mimann/wheeler-scratch/pacbio/run_2/uniques.fasta -threads 8  -blast6out zotus_duplicate_hits_pacbio.b6 -strand both -maxaccepts 500 -threads 8 -id 1.00


/users/mimann/usearch9 -usearch_global zotus_removed_duplicates.fa -db /users/mimann/wheeler-scratch/arctic/matching_long_reads/all_arctic_sanger.fasta -threads 8  -blast6out zotus_duplicate_hits_sanger.b6 -strand both -maxaccepts 500 -threads 8 -id 1.00

```

running uparse at 98% so i can use this for removing chimeras prior to clustering against Leho dataset
-------
rerunning clustering at 98%. i will need to download the uparse output and manually filter out the chimeras. Then I will 
```{bash   chimera_detection.slurm}


#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --mem=0
#SBATCH --time=4:00:00
#SBATCH --job-name=finding_chimeras

cd /users/mimann/wheeler-scratch/arctic/clustering


/users/mimann/usearch8.1.1861_i86linux32 -cluster_otus Arctic_Fungi_uniques.fa -uparseout uparse.txt -otu_radius_pct .98
```



```{r clean up fasta to remove chimeras}
library(microseq)
library(tidyverse)

uparse_calls <- read_table("Chimera_removal/uparse.txt", col_names = c("Read", "Call", "Percent", "Extra"))

unique_reads <- readFasta("Chimera_removal/Arctic_Fungi_uniques.fa")

# find all chimeras from uparse file
chimeras <- 
  uparse_calls %>%
  filter(Call != "otu" & Call != "match" )  %>%
  pull(Read)


# setting an arbiratry size so i can cluster these first. Then compare arctic reads to each of these
tedersoo_otus <- 
  readFasta("Tedersoo_dataset/Tedersoo_Fungi_GSMc_OTUs.fasta") %>%
  mutate(Header = str_remove(Header, pattern = "\\t.*")) %>%
  mutate(Header = paste0(Header, ";size=549193;"))
  

# remove chimeras from unique reads
# write to file 
tedersoo_otus %>%
  bind_rows(unique_reads) %>%
  filter(!(Header %in% chimeras)) %>%
  writeFasta("Chimera_removal/tedersoo_arctice_to_cluster_98.fasta")



```




```{bash   arctic_leho_cluster_98.slurm}


#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --mem=0
#SBATCH --cpus-per-task=8 
#SBATCH --time=48:00:00
#SBATCH --job-name=clustering_chimeras

cd /users/mimann/wheeler-scratch/arctic/clustering

/users/mimann/usearch8.1.1861_i86linux32 -cluster_smallmem tedersoo_arctice_to_cluster_98.fasta -id .98 -centroids otus_tedersoo_arctic_98.fasta -strand both -uc clusters.uc -sortedby size
```


```{bash otu_duplicates_98_check.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --job-name=find_duplicates_98_clustering


cd /users/mimann/wheeler-scratch/arctic/clustering


# checkign for duplicates that might be shorter but the same things. 
/users/mimann/usearch9 -usearch_global otus_tedersoo_arctic_98.fasta -db otus_tedersoo_arctic_98.fasta -id 0.98 -blast6out otus_98_duplicate_hits.b6 -strand both -maxaccepts 500 -threads 8

```



One tricky thing I am running into is Tedersoo's OTUs are not all trimed to the same length. Thus, some are lacking on the end of the ITS4 region. This creates a minor complication where I have partial sequences (Missing 5.8S region) but extend beyond the Tedersoo OTUs. Because of this, Removing the shorter reads might result in sequences that do not map back properly to Tedersoo's OTUS. 
instead, I will keep the rep seq with everything, (once I reorder it to match the input) and then afterwards use the otus_98_duplicate_hits.b6 file to merge the OTUs that are truly a subet of the larger OTU. I will be extra conservative and assume if it maps back on otus_98_duplicate_hits.b6 at 98%, then I will choose the longest (presumably the Tedersoo OTU) as the representative sequence. This should cut down on the number of OTUs and make it comparable across datasets. 


Reorder rep seq. Make sure it matches input file
```{r reorder otu rep seq}
library(microseq)
library(tidyverse)


otus_names <- 
  readFasta("OTUs/otus_tedersoo_arctic_98.fasta") %>%
  pull(Header)


# going to make sure OTU file is in the right order so using initial list 
# now i can use this to map seqs 
uniques <- readFasta("Chimera_removal/tedersoo_arctice_to_cluster_98.fasta")

uniques %>%
  filter(Header %in% otus_names) %>%
  writeFasta("OTUs/otus_tedersoo_arctic_98_ordered_properly.fasta")
  
```



Mapping read counts back to each sample
```{bash mapping_otus_98.slurm}
#!/bin/bash
#SBATCH --ntasks=8
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --job-name=mapping_otus_98


cd /users/mimann/wheeler-scratch/arctic/clustering

/users/mimann/usearch9 -usearch_global arctic_filtered.fasta -db otus_tedersoo_arctic_98_ordered_properly.fasta -strand both -id 0.98 -otutabout arctic_otu_table_98.txt
```



duplicate code

Read in hit data that the otus blasted against itself at 98% or higher. 
```{r read in data}
library(microseq)
library(tidyverse)
  # using blast6 column names (listed on usearch)
hits <- read_table("OTUs/otus_98_duplicate_hits.b6", 
                   col_names = c("Query_Label", "Target_Label", "Percent_Identity", "Alignment_Length", 
                                 "Mismatches", "Gaps", "Start_Query", "End_Query", "Start_Target", "End_Target", 
                                 "E_Value", "Bit_score"))

otus <- readFasta("OTUs/otus_tedersoo_arctic_98.fasta")

# going to make sure OTU file is in the right order so using initial list 
uniques <- readFasta("Chimera_removal/tedersoo_arctice_to_cluster_98.fasta")
```














Cleaning up hits that are likely due to using forward reads that are more abudnant than the longer full reads. 
Just a problem that occur with using a greedy algorithm. 
```{r clean up hits}

otu_read_length <- 
  otus %>%
  mutate(read_length = nchar(Sequence)) %>%
  select(-Sequence)
  
  



potential_duplicates <- 
  hits %>%
  filter(Query_Label != Target_Label)  %>% # remove hits to the same sequence. 
  left_join(otu_read_length, by = c("Query_Label" = "Header")) %>% 
  rename(Query_length = read_length) %>%
  left_join(otu_read_length, by = c("Target_Label" = "Header")) %>% 
  rename(Target_length = read_length) %>%
  filter(Query_length <= Target_length) %>% # only include hits with a shorter query seq than target seq
  filter(Alignment_Length / Query_length > .70) %>% # make sure alignment spans query 
  mutate(percent_coverage = Alignment_Length /End_Query) %>%
  View()
  
   %>% # find query seqs that are shorter than their targets. These probably need to be removed.
  pull(Query_Label)


otus %>%
  filter(!(Header %in% potential_duplicates)) %>% # remove duplicates
  writeFasta("ZOTUs/zotus_removed_duplicates.fa")
```










# in case i want an interactive node
salloc --cpus-per-task=8 --time=02:00:00 --partition=debug